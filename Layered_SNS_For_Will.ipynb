{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "509948ba",
   "metadata": {},
   "source": [
    "$$u_{k,t+1} = u_{k,t} + \\frac{dt}{C_{m,k}}\\big[-G_{mem,n}u_{k,t} + I_{k} + \\sum_{i\\in n} (G_{syn,i,k}(u_{k,t}) \\cdot (\\Delta E_{syn,i,k} - u_{i,t}))\\big]$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "0fe79aa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import gradcheck"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "c768ee0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GsynModule(nn.Module):\n",
    "    def __init__(self, n, activation_fn, G_min=None, G_max=None, G_scale=None):\n",
    "        super(GsynModule, self).__init__()\n",
    "        \n",
    "        n = (n, n) if type(n) is int else n\n",
    "        \n",
    "        # Default initialization\n",
    "        if G_min is None:\n",
    "            G_min = torch.rand(n, dtype=torch.double)\n",
    "        if G_max is None:\n",
    "            G_max = torch.rand(n, dtype=torch.double)\n",
    "        if G_scale is None:\n",
    "            G_scale = torch.rand(n, dtype=torch.double)\n",
    "        \n",
    "        self.G_min = nn.Parameter(G_min)\n",
    "        self.G_max = nn.Parameter(G_max)\n",
    "        self.G_scale = nn.Parameter(G_scale)\n",
    "        \n",
    "        assert G_min.size() == G_max.size() == G_scale.size()\n",
    "        self.activation_fn = activation_fn\n",
    "\n",
    "    def forward(self, u):\n",
    "        row_repeat_u = u.expand(*self.G_scale.size())\n",
    "        unscaled_u = (row_repeat_u - self.G_min) / (self.G_max - self.G_min)\n",
    "        activated_u = self.activation_fn(unscaled_u)\n",
    "        scaled_u = activated_u * (self.G_max - self.G_min) + self.G_min\n",
    "        return scaled_u * self.G_scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "6f4dae67",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConductanceLayerMulti(nn.Module):\n",
    "    def __init__(self, n, n_prev, dt=None,\n",
    "                 C_mem=None, G_mem=None, b_mem=None, \n",
    "                 Esyn_self=None, E_syn_prev=None,\n",
    "                 Gsyn_self=None, Gsyn_prev=None,\n",
    "                 is_first=False\n",
    "                ):\n",
    "        super(ConductanceLayerMulti, self).__init__()\n",
    "        \n",
    "        # Default initialization\n",
    "        if dt is None:\n",
    "            dt = torch.ones(n, dtype=torch.double)\n",
    "        if C_mem is None:\n",
    "            C_mem = torch.rand(n, dtype=torch.double)\n",
    "        if G_mem is None:\n",
    "            G_mem = torch.rand(n, dtype=torch.double)\n",
    "        if b_mem is None:\n",
    "            b_mem = torch.rand(n, dtype=torch.double)\n",
    "        if Esyn_self is None:\n",
    "            Esyn_self = torch.rand(n, n, dtype=torch.double)\n",
    "        if E_syn_prev is None:\n",
    "            E_syn_prev = torch.rand(n, n, dtype=torch.double)\n",
    "        \n",
    "        self.n = n\n",
    "        self.is_first = is_first\n",
    "        self.dt = dt\n",
    "        self.C_mem = nn.Parameter(C_mem)\n",
    "        self.G_mem = nn.Parameter(G_mem)\n",
    "        self.b_mem = nn.Parameter(b_mem)\n",
    "        self.Esyn_self = nn.Parameter(Esyn_self)\n",
    "        self.Esyn_prev = nn.Parameter(E_syn_prev)\n",
    "        \n",
    "        # Gsyn modules\n",
    "        if Gsyn_self is None:\n",
    "            self.Gsyn_self = GsynModule(n, activation_fn=lambda x: torch.clamp(x,min=0,max=1))\n",
    "        else:\n",
    "            self.Gsyn_self = Gsyn_self\n",
    "        \n",
    "        if Gsyn_prev is None:\n",
    "            self.Gsyn_prev = GsynModule((n,n_prev), activation_fn=lambda x: torch.clamp(x,min=0,max=1))\n",
    "        else:\n",
    "            self.Gsyn_prev = Gsyn_prev\n",
    "\n",
    "    def forward(self, u_self, u_prev):\n",
    "        row_repeat_u_self = u_self.expand(*u_self.size(), u_self.size(-1))\n",
    "        ds = self.Esyn_self - row_repeat_u_self\n",
    "        ds = torch.matmul(ds, self.Gsyn_self(u_self)) \n",
    "\n",
    "        if self.is_first:\n",
    "            dp = u_prev*torch.eye(u_prev.size(-1))\n",
    "        else:\n",
    "            dp = self.Esyn_prev - row_repeat_u_self\n",
    "            dp = torch.matmul(dp, self.Gsyn_prev(u_prev))\n",
    "\n",
    "        du = (-self.G_mem * u_self + self.b_mem + ds.sum(dim=1) + dp.sum(dim=1)) * (self.dt / self.C_mem)\n",
    "        return u_self + du"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "84e3412f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConductanceNetwork(nn.Module):\n",
    "    def __init__(self, layers: nn.ModuleList):\n",
    "        super(ConductanceNetwork, self).__init__()\n",
    "        self.layers = layers\n",
    "        #previous states are 1 indexed, and state '0' is used to store the inputs\n",
    "        self.current_states = [torch.zeros(layer.n) for layer in self.layers] # type: ignore\n",
    "        self.prev_states = [state.clone() for state in self.current_states]\n",
    "        self.prev_states.insert(0,torch.empty(1))\n",
    "    def forward(self,inp):\n",
    "        self.prev_states[0] = inp\n",
    "        for i,layer in enumerate(self.layers):\n",
    "            print(i,self.prev_states[i+1],self.prev_states[i])\n",
    "            self.current_states[i] = layer(self.prev_states[i+1],self.prev_states[i])\n",
    "        self.prev_states = self.current_states\n",
    "        return self.current_states[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "b364534a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 tensor([0., 0.]) tensor([ 0.3158, -1.0759], dtype=torch.float64)\n",
      "1 tensor([0., 0., 0., 0.]) tensor([0., 0.])\n",
      "2 tensor([0., 0.]) tensor([0., 0., 0., 0.])\n",
      "tensor([2.2783, 6.2844], dtype=torch.float64, grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "layer1 = ConductanceLayerMulti(n_prev=2,n=2,is_first=True)\n",
    "layer2 = ConductanceLayerMulti(n_prev=2, n=4)\n",
    "layer3 = ConductanceLayerMulti(n_prev=4, n=2)\n",
    "\n",
    "# Create the ConductanceNetwork with the 3 layers\n",
    "layers = nn.ModuleList([layer1, layer2, layer3])\n",
    "network = ConductanceNetwork(layers)\n",
    "\n",
    "# Test the network with a random input\n",
    "input_tensor = torch.randn(2, dtype=torch.double)\n",
    "output = network(input_tensor)\n",
    "print(output)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (Trainable SNS)",
   "language": "python",
   "name": "trainable-sns"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
